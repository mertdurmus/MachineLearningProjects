```{r}
library(dplyr)
library(tidyverse)
library(AppliedPredictiveModeling)
library(pls)
library(elasticnet)
library(broom) 
library(glmnet)
library(MASS)
library(ISLR)
library(PerformanceAnalytics)
library(funModeling)
library(Matrix)
library(caret)
require(devtools)
library(magrittr) 
library(dplyr)    
library("Hmisc")
library(corrplot)
setwd("C:/Users/mert5/Desktop/4-1 dönem/machine learning/proje başlama/ikinci çalışma")
#%b %d, %Y
```

```{r}
df<- Assets2
head(df)
df$name <- NULL
df$date <-NULL
df$`Short-term investments`<-NULL
df$`Inventories, excludes inventories classified in Other assets`<-NULL
df$`Short-term investments`<-NULL
df<- na.omit(df)
df$`Cash and cash equivalents` <- as.numeric(df$`Cash and cash equivalents`)
```



```{r}

st_z <- function(x) {
  
  (x - mean(x)) / sd(x)

}



st_sifir_bir <- function(x) {
  
  (x-min(x))/(max(x)-min(x))
  
}

df<- df %>% mutate_all(st_sifir_bir)


df<- df %>% mutate_all(st_z)
```

#splitting
```{r}
set.seed(3456)
train_indeks <- createDataPartition(df$price, 
                                    p = .7, 
                                    list = FALSE, 
                                    times = 1)


train <- df[train_indeks, ]
test <- df[-train_indeks, ]

train_x <- train %>% dplyr::select(-price)
train_y <- train$price

test_x <- test %>% dplyr::select(-price)
test_y <- test$price
#tek bir veri seti 
training <- data.frame(train_x, price = train_y)
```




```{r}
df<-df %>% 
  rename(
    nakit='Cash and cash equivalents',
    net='Accounts receivable. net of allowance for doubtful accounts',
    stok='Inventories. excludes inventories classified in Other assets',
    aktifler='Current assets',
    mulk ='Property. plant and equipment. at cost. net of accumulated depreciation',
    dgr_vrl = 'Other assets',
    imaj ='Goodwill',
    alacak = 'Noncurrent assets',
    tpl_net ='Total assets'
    )





```



```{r}
head(df)
save(df,file="data.Rda")
load("data.Rda")
```



```{r}
#lineer regresyon

lm_fit <- lm(price ~ ., data = training)
summary(lm_fit)

plot(lm_fit)

defaultSummary(data.frame(obs = training$prc,pred = lm_fit$fitted.values))

#lineer regresyon model tuning

ctrl <- trainControl(method = "cv", 
                     number = 10)

lm_val_fit <- train(x = train_x, y = train_y,
      method = "lm",
      trControl = ctrl)


lm_val_fit$results

summary(lm_val_fit)
names(lm_val_fit)
lm_val_fit$finalModel



lm_model <- lm(price~ net+aktifler+tpl_net, data = training)
summary(lm_model)
```







#pcr regresyon
```{r}
pcr_fit <- pcr(price~., data = training,
    scale = TRUE,
    validation = "CV")

pcr_fit
plot(pcr_fit)

summary(pcr_fit)
validationplot(pcr_fit, val.type = "MSEP")

names(pcr_fit)

pcr_fit$scores

defaultSummary(data.frame(obs = training$price,
pred = as.vector(pcr_fit$fitted.values))
)
#        RMSE     Rsquared          MAE 
#4.745344e+04 4.435749e-02 3.295026e+04 
# RMSE     Rsquared          MAE 
#172.02154661   0.07520768 104.29895145 

#model tuning

ctrl <- trainControl(method = "cv", number = 10)
set.seed(100)

pcr_tune <- train(train_x, train_y,
                  method = "pcr",
                  trControl = ctrl,
                  preProc = c("center", "scale"))
#model ciktisi
pcr_tune

pcr_tune <- train(train_x, train_y,
                  method = "pcr",
                  trControl = ctrl,
                  tuneLength = 20,
                  preProc = c("center", "scale"))

pcr_tune

plot(pcr_tune)

pcr_tune$finalModel


#test hatası

defaultSummary(data.frame(obs = test_y,
pred = as.vector(predict(pcr_tune, test_x)))
)
#    RMSE     Rsquared          MAE 
#5.350895e+04 3.355511e-02 3.727612e+04 

#      RMSE     Rsquared          MAE 
#164.55238562   0.01410787  96.91335055 
```






#gbm model
```{r}
#install.packages("rsample")
#install.packages("gbm")
#install.packages("xgboost")
#install.packages("h2o")
#install.packages("pdp")
#install.packages("lime")
library(rsample)      # data splitting 
library(gbm)          # basic implementation
library(xgboost)      # a faster implementation of gbm
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # a java-based platform
library(pdp)          # model visualization
library(ggplot2)      # model visualization
library(lime)         # model visualization

set.seed(123)

# train GBM model
gbm.fit <- gbm(
  formula = price ~ .,
  distribution = "gaussian",
  data = training,
  n.trees = 10000,
  interaction.depth = 1,
  shrinkage = 0.001,
  cv.folds = 5,
  n.cores = NULL, 
  verbose = FALSE
  )  

defaultSummary(data.frame(obs=train_y,pred=gbm.fit$fit))

print(gbm.fit)

summary(gbm.fit)
```


```{r}
# get MSE and compute RMSE
sqrt(min(gbm.fit$cv.error))
## [1] 0.1711839

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit, method = "cv")
```


#model tuning

```{r}

# create hyperparameter grid
hyper_grid <- expand.grid(
  shrinkage = c(.01, .1, .3, .5),
  interaction.depth = c(1, 3,5),
  n.minobsinnode = c(5, 10),
  bag.fraction = c(.65, .8), 
  optimal_trees = 0,               # a place to dump results
  min_RMSE = 0                     # a place to dump results
)
# randomize data
random_index <- sample(1:nrow(training), nrow(training))
random_ames_train <- training[random_index, ]

# grid search 
for(i in 1:nrow(hyper_grid)) {
  
  # reproducibility
  set.seed(123)
  
  # train model
  gbm.tune <- gbm(
    formula = price ~ .,
    distribution = "gaussian",
    data = random_ames_train,
    n.trees = 6000,
    interaction.depth = hyper_grid$interaction.depth[i],
    shrinkage = hyper_grid$shrinkage[i],
    n.minobsinnode = hyper_grid$n.minobsinnode[i],
    bag.fraction = hyper_grid$bag.fraction[i],
    train.fraction = .75,
    n.cores = NULL, # will use all cores by default
    verbose = FALSE
  )
  
  # add min training error and trees to grid
  hyper_grid$optimal_trees[i] <- which.min(gbm.tune$valid.error)
  hyper_grid$min_RMSE[i] <- sqrt(min(gbm.tune$valid.error))
}

hyper_grid %>% 
  dplyr::arrange(min_RMSE) %>%
  head(10)
```


#final model
```{r}
# for reproducibility
set.seed(123)

# train GBM model
gbm.fit.final <- gbm(
  formula = price ~ .,
  distribution = "gaussian",
  data = training,
  n.trees = 52,
  interaction.depth = 3,
  shrinkage = 0.3,
  n.minobsinnode = 5,
  bag.fraction = .80, 
  train.fraction = 1,
  n.cores = NULL,
  verbose = FALSE,
  cv.folds = 5
  )  
```

```{r}
defaultSummary(data.frame(obs=train_y,pred=gbm.fit.final$fit))
```


#visualizing
```{r}
par(mar = c(5, 8, 1, 1))
summary(
  gbm.fit.final, 
  cBars = 10,
  method = relative.influence, # also can use permutation.test.gbm
  las = 2
  )
```



```{r}
# get MSE and compute RMSE
sqrt(min(gbm.fit.final$cv.error))
## [1] 0.1711839

# plot loss function as a result of n trees added to the ensemble
gbm.perf(gbm.fit.final, method = "cv")
```


#predicting
```{r}
# predict values for test data
pred <- predict(gbm.fit.final, n.trees = gbm.fit.final$n.trees, test)

# results
caret::RMSE(pred, test$price)
## [1] 20681.88
```



#xgb model
```{r}
# variable names
#install.packages("vthreat")
#library("vtreat")
features <- setdiff(names(training), "price")

# Create the treatment plan from the training data
#treatplan <- vtreat::designTreatmentsZ(training, features, verbose = FALSE)

# Get the "clean" variable names from the scoreFrame
#new_vars <- treatplan

# Prepare the training data
features_train <- data.matrix(training, rownames.force = NA)
response_train <- training$price

# Prepare the test data
features_test <- data.matrix(test, rownames.force = NA)
response_test <- test$price

# dimensions of one-hot encoded data
dim(features_train)
## [1] 2051  208
dim(features_test)
## [1] 879 208
# reproducibility
set.seed(123)
#response_train <- training$price
xgb.fit1 <- xgb.cv(
  data = features_train,
  label = response_train,
  nrounds = 10000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0               # silent,
)
```


```{r}
# get number of trees that minimize error
xgb.fit1$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean),
  )
##   ntrees.train rmse.train ntrees.test rmse.test
## 1          965  0.5022836          60  27572.31

# plot error vs number trees
ggplot(xgb.fit1$evaluation_log) +
  geom_line(aes(iter, train_rmse_mean), color = "red") +
  geom_line(aes(iter, test_rmse_mean), color = "blue")
```













```{r}
# create parameter list
  params <- list(
    eta = .1,
    max_depth = 5,
    min_child_weight = 2,
    subsample = .8,
    colsample_bytree = .9
  )

# reproducibility
set.seed(123)

# train model
xgb.fit3 <- xgb.cv(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1000,
  nfold = 5,
  objective = "reg:linear",  # for regression models
  verbose = 0,               # silent,
  early_stopping_rounds = 10 # stop if no improvement for 10 consecutive trees
)

# assess results
xgb.fit3$evaluation_log %>%
  dplyr::summarise(
    ntrees.train = which(train_rmse_mean == min(train_rmse_mean))[1],
    rmse.train   = min(train_rmse_mean),
    ntrees.test  = which(test_rmse_mean == min(test_rmse_mean))[1],
    rmse.test   = min(test_rmse_mean),
  )
##   ntrees.train rmse.train ntrees.test rmse.test
## 1          180   5891.703         170  24650.17
```

```{r}
# parameter list
params <- list(
  eta = 0.01,
  max_depth = 5,
  min_child_weight = 5,
  subsample = 0.65,
  colsample_bytree = 1
)

# train final model
xgb.fit.final <- xgboost(
  params = params,
  data = features_train,
  label = response_train,
  nrounds = 1576,
  objective = "reg:linear",
  verbose = 0
)
```



```{r}
# create importance matrix
importance_matrix <- xgb.importance(model = xgb.fit.final)

# variable importance plot
xgb.plot.importance(importance_matrix, top_n = 10, measure = "Gain")
```









```{r}
df2=nvida
head(df2)
df2$name <- NULL
df2$date <-NULL

df2<- df2 %>% mutate_all(st_sifir_bir)
set.seed(3456)
train_indeks <- createDataPartition(df2$price, 
                                    p = .7, 
                                    list = FALSE, 
                                    times = 1)
train <- df2[train_indeks, ]
test <- df2[-train_indeks, ]

train_x <- train %>% dplyr::select(-price)
train_y <- train$price

test_x <- test %>% dplyr::select(-price)
test_y <- test$price
#tek bir veri seti 
training <- data.frame(train_x, price = train_y)

```


```{r}
lm_fit <- lm(price ~ ., data = training)
summary(lm_fit)

plot(lm_fit)

```




```{r}
library(rsample)     
library(gbm)         
library(xgboost)      
library(caret)       
library(h2o)        
library(pdp)        
library(ggplot2)     
library(lime)        
set.seed(123)

# train GBM model
gbm.fit <- gbm(
  formula = price ~ .,
  distribution = "gaussian",
  data = training,
  n.trees = 100,
  interaction.depth = 1,
  shrinkage = 0.1,
  cv.folds = 2,
  n.cores = NULL, 
  verbose = FALSE
  )  


print(gbm.fit)
```















